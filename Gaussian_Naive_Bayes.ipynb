{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CJu6RLzU_uEm"
   },
   "source": [
    "# **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9pZFnF4Z_ft1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IPZYJXKs_xb7"
   },
   "source": [
    "# **Functions Definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yrBy_0PW_su1"
   },
   "outputs": [],
   "source": [
    "def gaussian_function(x,m,s):\n",
    "  x = np.array(x);\n",
    "  m = np.array(m);\n",
    "  s = np.array(s);\n",
    "  k = m.size;\n",
    "  factor = 1/np.sqrt(((2*np.pi)**k)*np.linalg.det(s));\n",
    "  ret_arr = []\n",
    "  for i in range(x.shape[0]):\n",
    "    exponent = (-0.5) * np.matmul(np.matmul((x[i]-m),np.linalg.inv(s)),(x[i]-m).T);\n",
    "    ret_arr.append(np.exp(exponent))\n",
    "\n",
    "  return factor*np.array(ret_arr)\n",
    "\n",
    "def naive_bayes_classifier(x,name=None):\n",
    "  x = np.array(x)\n",
    "  z = []\n",
    "  if(name == None):\n",
    "    for i in range(x.shape[0]):\n",
    "      z.append(np.argmax(x[i])+1)\n",
    "  else:\n",
    "    for i in range(x.shape[0]):\n",
    "      z.append(name[np.argmax(x[i])])\n",
    "  return z\n",
    "\n",
    "def loss_function(x,y):\n",
    "  x = np.array(x)\n",
    "  y = np.array(y)\n",
    "  if(x.size == y.size):\n",
    "    L = np.zeros(x.size)\n",
    "    for i in range(x.size):\n",
    "      if(x[i] == y[i]):\n",
    "        L[i] = 0;\n",
    "      else:\n",
    "        L[i] = 1;\n",
    "    return L;\n",
    "  else:\n",
    "    print(\"Input arrays with same dimensions\")   \n",
    "    return \n",
    "  \n",
    "def MLE_BDT(P,p): # P = [0.7,0.3], p = [1/3,1/3,1/3]\n",
    "  print(\"Training dataset size = {}% and testing dataset size = {}%\".format(P[0]*100,P[1]*100))\n",
    "  print(\"Prior distribution = {}\".format(p))\n",
    "\n",
    "  df = pd.read_csv(\"irisdata.csv\", sep=',')\n",
    "\n",
    "  attributes = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"Class\"]\n",
    "  df.columns = attributes\n",
    "  #train_df = df.sample(frac=P[0])\n",
    "  #test_df = df.sample(frac=P[1])\n",
    "\n",
    "  classes = df['Class'].unique().tolist()\n",
    "  for cls, df_cls in df.groupby('Class'):\n",
    "    if(cls == classes[0]):\n",
    "      df_0 = df_cls\n",
    "    elif(cls == classes[1]):\n",
    "      df_1 = df_cls\n",
    "    elif(cls == classes[2]):\n",
    "      df_2 = df_cls\n",
    "\n",
    "  \n",
    "  train_df_0, test_df_0 = train_test_split(df_0, test_size=P[1])\n",
    "  train_df_1, test_df_1 = train_test_split(df_1, test_size=P[1])\n",
    "  train_df_2, test_df_2 = train_test_split(df_2, test_size=P[1])\n",
    "\n",
    "  train_df = pd.concat([train_df_0, train_df_1, train_df_2], ignore_index=True)\n",
    "  test_df = pd.concat([test_df_0, test_df_1, test_df_2], ignore_index=True)\n",
    "  \n",
    "  m = np.array([train_df_0.drop([\"Class\"],axis=1).mean().to_numpy(),\\\n",
    "    train_df_1.drop([\"Class\"],axis=1).mean().to_numpy(),\\\n",
    "    train_df_2.drop([\"Class\"],axis=1).mean().to_numpy()])\n",
    "\n",
    "  print(m)\n",
    "  s = np.array([train_df_0.drop([\"Class\"],axis=1).cov().to_numpy(),\\\n",
    "    train_df_1.drop([\"Class\"],axis=1).cov().to_numpy(),\\\n",
    "    train_df_2.drop([\"Class\"],axis=1).cov().to_numpy()])\n",
    "\n",
    "  p_x_0 = gaussian_function(train_df.drop([\"Class\"],axis=1),m[0],s[0])\n",
    "  p_x_1 = gaussian_function(train_df.drop([\"Class\"],axis=1),m[1],s[1])\n",
    "  p_x_2 = gaussian_function(train_df.drop([\"Class\"],axis=1),m[2],s[2])\n",
    "  train_df[\"P(X|0)\"] = p_x_0;\n",
    "  train_df[\"P(X|1)\"] = p_x_1;\n",
    "  train_df[\"P(X|2)\"] = p_x_2;\n",
    "\n",
    "  lp_x_0 = np.log(p_x_0)\n",
    "  lp_x_1 = np.log(p_x_1)\n",
    "  lp_x_2 = np.log(p_x_2)\n",
    "  print(np.sum(lp_x_0),np.sum(lp_x_1),np.sum(lp_x_2))\n",
    "  p_0_x = (p_x_0*p[0])/(p_x_0*p[0] + p_x_1*p[1] + p_x_2*p[2])\n",
    "  p_1_x = (p_x_1*p[1])/(p_x_0*p[0] + p_x_1*p[1] + p_x_2*p[2])\n",
    "  p_2_x = (p_x_2*p[2])/(p_x_0*p[0] + p_x_1*p[1] + p_x_2*p[2])\n",
    "  train_df[\"P(0|X)\"] = p_0_x;\n",
    "  train_df[\"P(1|X)\"] = p_1_x;\n",
    "  train_df[\"P(2|X)\"] = p_2_x;\n",
    "\n",
    "  train_df[\"Prediction\"] = naive_bayes_classifier(\\\n",
    "    np.column_stack((p_0_x,p_1_x,p_2_x)),classes)\n",
    "\n",
    "  train_df[\"Loss\"] = loss_function(\\\n",
    "    train_df[\"Prediction\"].to_numpy(),train_df[\"Class\"].to_numpy())\n",
    "\n",
    "  for loss, df_loss in train_df.groupby('Loss'):\n",
    "    if(loss == 0):\n",
    "      loss_df_0 = df_loss\n",
    "    elif(loss == 1):\n",
    "      loss_df_1 = df_loss\n",
    "\n",
    "  print(\"Accuracy of model on training set is: {}%\".format((1-train_df[\"Loss\"].mean())*100))\n",
    "\n",
    "  p_x_0 = gaussian_function(test_df.drop([\"Class\"],axis=1),m[0],s[0])\n",
    "  p_x_1 = gaussian_function(test_df.drop([\"Class\"],axis=1),m[1],s[1])\n",
    "  p_x_2 = gaussian_function(test_df.drop([\"Class\"],axis=1),m[2],s[2])\n",
    "\n",
    "  test_df[\"P(X|0)\"] = p_x_0;\n",
    "  test_df[\"P(X|1)\"] = p_x_1;\n",
    "  test_df[\"P(X|2)\"] = p_x_2;\n",
    "  \n",
    "  p_0_x = (p_x_0*p[0])/(p_x_0*p[0] + p_x_1*p[1] + p_x_2*p[2])\n",
    "  p_1_x = (p_x_1*p[1])/(p_x_0*p[0] + p_x_1*p[1] + p_x_2*p[2])\n",
    "  p_2_x = (p_x_2*p[2])/(p_x_0*p[0] + p_x_1*p[1] + p_x_2*p[2])\n",
    "  test_df[\"P(0|X)\"] = p_0_x;\n",
    "  test_df[\"P(1|X)\"] = p_1_x;\n",
    "  test_df[\"P(2|X)\"] = p_2_x;\n",
    "\n",
    "  test_df[\"Prediction\"] = naive_bayes_classifier(np.column_stack((p_0_x,p_1_x,p_2_x)),classes)\n",
    "\n",
    "  test_df[\"Loss\"] = loss_function(test_df[\"Prediction\"].to_numpy(),test_df[\"Class\"].to_numpy())\n",
    "\n",
    "  print(\"Accuracy of model on test set is: {}% \\n\".format((1-test_df[\"Loss\"].mean())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "54siWSa9AfLs"
   },
   "source": [
    "# **Model Training and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "YpvDA6GG_2ki",
    "outputId": "768be2e4-7e31-4954-c5cc-1b0a008f9074"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size = 70.0% and testing dataset size = 30.0%\n",
      "Prior distribution = [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n",
      "[[5.04285714 3.5        1.47142857 0.24857143]\n",
      " [5.92571429 2.78       4.27142857 1.34285714]\n",
      " [6.58285714 3.         5.51142857 2.00571429]]\n",
      "-16854.93899231041 -2621.407256684148 -4507.260075848843\n",
      "Accuracy of model on training set is: 98.09523809523809%\n",
      "Accuracy of model on test set is: 100.0% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "MLE_BDT([0.7,0.3],[1/3,1/3,1/3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 992
    },
    "colab_type": "code",
    "id": "ZrcW68Qb_7RK",
    "outputId": "fc181839-4312-4c00-fe80-666d44c3dea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size = 70.0% and testing dataset size = 30.0%\n",
      "Prior distribution = [0.33333333 0.33333333 0.33333333]\n",
      "[[5.06857143 3.44857143 1.47714286 0.25714286]\n",
      " [5.87428571 2.77714286 4.19428571 1.32285714]\n",
      " [6.54       2.97714286 5.52       1.99714286]]\n",
      "-19988.703873392686 -2612.398717582956 -3657.6117376971906\n",
      "Accuracy of model on training set is: 99.04761904761905%\n",
      "Accuracy of model on test set is: 97.77777777777777% \n",
      "\n",
      "Training dataset size = 70.0% and testing dataset size = 30.0%\n",
      "Prior distribution = [0.3 0.5 0.2]\n",
      "[[5.00857143 3.42857143 1.48       0.25142857]\n",
      " [5.89142857 2.77714286 4.21428571 1.31428571]\n",
      " [6.62857143 2.94       5.59142857 2.00571429]]\n",
      "-16200.722320815657 -2553.9515742276467 -3057.6681326850207\n",
      "Accuracy of model on training set is: 99.04761904761905%\n",
      "Accuracy of model on test set is: 97.77777777777777% \n",
      "\n",
      "Training dataset size = 50.0% and testing dataset size = 50.0%\n",
      "Prior distribution = [0.33333333 0.33333333 0.33333333]\n",
      "[[4.964 3.4   1.424 0.232]\n",
      " [5.976 2.748 4.224 1.304]\n",
      " [6.652 2.964 5.536 2.   ]]\n",
      "-12805.584755859702 -1340.0514660086576 -2327.7952411785295\n",
      "Accuracy of model on training set is: 100.0%\n",
      "Accuracy of model on test set is: 97.33333333333334% \n",
      "\n",
      "Training dataset size = 50.0% and testing dataset size = 50.0%\n",
      "Prior distribution = [0.3 0.5 0.2]\n",
      "[[4.924 3.356 1.452 0.216]\n",
      " [5.964 2.828 4.296 1.332]\n",
      " [6.56  2.96  5.564 2.06 ]]\n",
      "-inf -1851.048082447509 -4075.7162855399797\n",
      "Accuracy of model on training set is: 100.0%\n",
      "Accuracy of model on test set is: 94.66666666666667% \n",
      "\n",
      "Training dataset size = 30.0% and testing dataset size = 70.0%\n",
      "Prior distribution = [0.33333333 0.33333333 0.33333333]\n",
      "[[5.12666667 3.47333333 1.41333333 0.25333333]\n",
      " [6.03333333 2.81333333 4.26       1.33333333]\n",
      " [6.68       3.01333333 5.64666667 1.93333333]]\n",
      "-7270.731506566447 -1469.033491296928 -1202.5183568701118\n",
      "Accuracy of model on training set is: 100.0%\n",
      "Accuracy of model on test set is: 96.19047619047619% \n",
      "\n",
      "Training dataset size = 30.0% and testing dataset size = 70.0%\n",
      "Prior distribution = [0.3 0.5 0.2]\n",
      "[[5.05333333 3.46666667 1.44666667 0.22666667]\n",
      " [6.02666667 2.69333333 4.29333333 1.30666667]\n",
      " [6.75333333 2.94       5.62666667 1.97333333]]\n",
      "-7194.382276932332 -1581.4781370336493 -1236.225515768637\n",
      "Accuracy of model on training set is: 100.0%\n",
      "Accuracy of model on test set is: 97.14285714285714% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:80: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "P = np.array([[0.7,0.3],[0.5,0.5],[0.3,0.7]])\n",
    "p = np.array([[1/3,1/3,1/3],[0.3,0.5,0.2]])\n",
    "for i in range(P.shape[0]):\n",
    "  for j in range(p.shape[0]):\n",
    "    MLE_BDT(P[i],p[j])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZU6wYMVFBEUg"
   },
   "source": [
    "# **Inference**\n",
    "\n",
    "1. When dataset distribution is 50% and priors are all equal we get train and test accuracies almost same.\n",
    "\n",
    "2. When train set is 30%  and priors are all equal we get better accuracy on train set at the cost of accuracy on the test set which is lower. \n",
    "\n",
    "3. When train set is smaller and but prior probabilities are different we have poor performance on test set (general trend).\n",
    "\n",
    "4. When train set 70% and priors are different we get very good estimation on test set because of the small numbers on test case.\n",
    "\n",
    "5. So optimal situation is to split the data in 50% and prior probabilities are same."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Question_6.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
